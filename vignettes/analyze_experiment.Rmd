---
title: "Analyze an entire experiment"
author: "Alex Holcombe"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
      fig_caption: yes
      fig_width: 7
      fig_height: 6 
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Read in the data, and rename some of the fields
```{r}
library(mixRSVP)
data <- backwards2_E1
#.mat file been preprocessed into melted long dataframe
numItemsInStream<- length( data$letterSeq[1,] )  

library(dplyr)

#To use dplyr operations, each column must be a 1d atomic vector or a list. So, can't have array fields like letterSeq
data$letterSeq<- NULL

#Give conditions better names than 1 and 2
names(data)[names(data) == 'target'] <- 'stream'
data <- data %>% mutate( stream =ifelse(stream==1, "Left","Right") )
#mutate condition to Orientation
names(data)[names(data) == 'condition'] <- 'orientation'
data <- data %>% mutate( orientation =ifelse(orientation==1, "Canonical","Inverted") )
```

Investigate why whole-experiment fitting not working by trying to fit some individual conditions
```{r}
df<-data %>% dplyr::filter(subject=="AA",orientation=="Canonical",stream=="Left")

analyzeOneCondition(df,numItemsInStream,parameterBounds(), nReplicates=3)

dataSmall <- data %>% dplyr::filter(subject=="AA",orientation=="Inverted")

estimates<- dataSmall %>%  #  filter(subject=="CB") %>%
    group_by_(.dots = condtnVariableNames) %>%  #.dots needed when you have a variable containing multiple factor names
    do( analyzeOneConditionDF(.,numItemsInStream,parameterBounds(), nReplicates=3) 
                     )

```
Fit mixture model to data.

```{r}

condtnVariableNames <- c("subject","orientation", "stream") # 

#Check whether already have parameter estimates or instead need to do it
calculate<-FALSE
if (!exists("estimates") | length(estimates)==0) { 
  calculate<-TRUE
}

if (calculate) {
  estimates<- data %>%  #  filter(subject=="CB") %>%
    group_by_(.dots = condtnVariableNames) %>%  #.dots needed when you have a variable containing multiple factor names
    do(  analyzeOneConditionDF(.,numItemsInStream,parameterBounds(), nReplicates=3)  )
}
head(estimates)
```

Plot the histogram and fit for one subject*condition.

* yellow = guessing component 
* light blue = Gaussian component
* green = sum of the guessing and Gaussian components. In other words, the histogram heights predicted by the model
* dark blue = continuous Gaussian. This helps get a sense of the effect of discretising the Gaussian. For instance, it's possible  for the Gaussian peak to rise high above the bars and still fit the discrete bins, suggesting  undesirably high estimates of the efficacy (likely accompanied by an undesirably low precision)

For goodness of fit, lower neg log likelihood means better fit.

```{r plotOneS, echo=FALSE, message=FALSE}

dCB<- dplyr::filter(data,subject=="BS",orientation=="Inverted",stream=="Left")  
minSPE<- -17; maxSPE<- 17
plotContinuousGaussian<-TRUE; annotateIt<-TRUE
g<- plot_hist_with_fit(dCB,minSPE,maxSPE,dCB$targetSP,numItemsInStream,plotContinuousGaussian,annotateIt, FALSE)
g + annotate("text", x = 12, y = 25, label = "CB, Inverted, Left stream")

```

Plot all data. First column is left stream. Second column is right stream. Each subject gets a pair of rows, "1" for upright letter trials, "2" for inverted letter trials.

```{r, echo=FALSE, message=TRUE, fig.height=36, fig.width=10}
#want fig.height of 10 per subject

library(dplyr)

#Add R parameter estimates to dataframe
df<- merge(data,estimates) 

curves<- df %>% group_by_at(.vars = condtnVariableNames) %>% 
  do(calcCurvesDataframes(.,minSPE,maxSPE,numItemsInStream))

#Calc numObservations to each condition. This is needed only for scaling the fine-grained Gaussian
#Calc the number of observations for each condition, because gaussianScaledforData needs to know.
dfGroups<- df %>% group_by_at(.vars = condtnVariableNames) %>% summarise(nPerCond = n())
#add nPerCond back to parameter estimates
estimates<- merge(estimates,dfGroups)
grain<-.05
gaussFine<- estimates %>% group_by_at(.vars = condtnVariableNames) %>% do(
  gaussianScaledFromDataframe(.,minSPE,maxSPE,grain) )


#PLOT EVERYTHING
g=ggplot(df, aes(x=SPE)) + facet_grid(subject+condition~target,  scales="free_y")
g<-g+geom_histogram(binwidth=1,color="grey90") + xlim(minSPE,maxSPE)
g<-g +theme_apa() #+theme(panel.grid.minor=element_blank(),panel.grid.major=element_blank())# hide all gridlines.
#g<-g+ theme(line=element_blank(), panel.border = element_blank())
sz=.8
#Plot the underlying Gaussian , not just the discretized Gaussian. But it's way too tall. I don't know if this is 
#a scaling problem or what actually is going on.
#g<-g + geom_line(data=gaussFine,aes(x=x,y=gaussianFreq),color="darkblue",size=1.2)

g<-g+ geom_point(data=curves,aes(x=x,y=combinedFitFreq),color="chartreuse3",size=sz*2.5)
g<-g+ geom_line(data=curves,aes(x=x,y=guessingFreq),color="yellow",size=sz)
#Discretized Gaussian
g<-g+ geom_line(data=curves,aes(x=x,y=gaussianFreq),color="lightblue",size=sz)

numGroups<- length(table(df$condition,df$subject,df$target))
fontSz = 5 #100/numGroups
g<-g + geom_text(data=curves,aes(x=-9,y=32, label = paste("--logLik==", round(val,1), sep = "")),  parse = TRUE,size=fontSz) +
  geom_text(data=curves,aes(x=-7,y=28, label = paste("plain(e)==", round(efficacy,2), sep = "")),  parse = TRUE,size=fontSz) +
  geom_text(data=curves,aes(x=-7,y=25, label = paste("mu==", round(latency,2), sep = "")),  parse = TRUE,size=fontSz)+
  geom_text(data=curves,aes(x=-7,y=22, label = paste("sigma==", round(precision,2), sep = "")),  parse = TRUE,size=fontSz)
show(g)
```
